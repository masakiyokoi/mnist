{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../common')\n",
    "from layers import *\n",
    "from functions import softmax,zscore,cross_entropy_error  #softmax(活性化関数),エントロピー二乗誤差(誤差関数)\n",
    "from gradient import numerical_gradient #numerical_gradient(パラメータの更新、勾配)import pprint\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重みWはガウス分布で初期化,バイアスは0で初期化\n",
    "class Twolayer:\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std = 0.01):\n",
    "        \n",
    "        #重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size) #学習率(0.01)*ランダムに生成された行列(input_size×hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size) #(1×hidden_size)、要素が0の1次元配列\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size) #行列の形が違う。\n",
    "        self.params['b2'] = np.zeros(output_size) #行列の形が違う。\n",
    "        \n",
    "        #レイヤの生成\n",
    "        self.layers = OrderedDict() #辞書作成\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def convert_t(self,t_train):\n",
    "   \n",
    "        t = np.zeros((t_train.shape[0],10))\n",
    "        for i in range(t_train.shape[0]):\n",
    "            label = t_train[i] #5\n",
    "            t[i][label-1] = 1\n",
    "        \n",
    "        return t\n",
    "        \n",
    "    \n",
    "    def predict(self,x): #layersを使うことでAffine,Relu,Affineと順序的に計算してくれる\n",
    "        \n",
    "        for layer in self.layers.values(): #layers.value()はlayersのリストの値を与えている。\n",
    "            #print(x.shape)\n",
    "            x = layer.forward(x) #Affine1,Relu,Affine2の順\n",
    "           \n",
    "            \n",
    "        return x\n",
    "         \n",
    "    \n",
    "    def loss(self,x,t): #lossの算出 by cross_entropy_error\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y,t) #Affine,Relu,Affineから得られた出力をsoftmax関数にかけて確率だしてそのエントロピー誤差を返す。\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        \n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis = 1)\n",
    "        if t.ndim != 1 : t = np.argmax(t,axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W:self.loss(x,t) #入力と正解ラベルのlossを求める無名関数loss_Wの作成\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self,x,t): #入力、教師ラベル\n",
    "        \n",
    "        #forward:順伝搬では今まで通りlossを算出する。\n",
    "        self.loss(x,t) #lossの中でpredictが呼ばれる。今、lossがわかっている。\n",
    "        \n",
    "        #backward:逆伝搬\n",
    "        dout = 1\n",
    "        dout  = self.lastLayer.backward(dout) #lastlayerはsmwlクラスのオブジェクト. smwlのbackwardの出力を返す。つまり、データ一個あたりの誤差。\n",
    "        #print(dout)\n",
    "        \n",
    "        layers = list(self.layers.values()) #辞書layersの内容でリスト作成。[Affine1,Relu,Affine2]\n",
    "        #print(layers)\n",
    "        layers.reverse() #リストの要素を反転 [Affine2,Relu,Affine1]\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout) #doutに各層(Affine2,Relu,Affine1)で微分した結果が入る。\n",
    "            #print(dout)\n",
    "        \n",
    "        #print(dout.shape)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../')\n",
    "from load_mnist import load_mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14005 0.1411\n",
      "0.9394333333333333 0.9383\n",
      "0.9576833333333333 0.9557\n",
      "0.9683 0.9627\n",
      "0.9738666666666667 0.9659\n",
      "0.97795 0.9697\n",
      "0.97915 0.9683\n",
      "0.9823166666666666 0.9715\n",
      "0.9843166666666666 0.973\n",
      "0.9862833333333333 0.9731\n",
      "0.9873166666666666 0.974\n",
      "0.9871333333333333 0.9722\n",
      "0.9894333333333334 0.9721\n",
      "0.985 0.9702\n",
      "0.9925333333333334 0.9741\n",
      "0.9919833333333333 0.9744\n",
      "0.99245 0.9743\n"
     ]
    }
   ],
   "source": [
    "'''(x_train,t_train), (x_test,t_test)= load_mnist(normalize=True,one_hot_label = True)'''\n",
    "\n",
    "x_train,t_train = load_mnist('',kind = 'train')\n",
    "x_test,t_test = load_mnist('',kind = 't10k')\n",
    "\n",
    "x_train = zscore(x_train)\n",
    "x_test = zscore(x_test)\n",
    "\n",
    "network = Twolayer(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "t_train = network.convert_t(t_train)\n",
    "t_test = network.convert_t(t_test)\n",
    "\n",
    "#print(x_train[0])\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size,1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size) #60000の中から100個\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch,t_batch) #誤差逆伝搬法で勾配求める\n",
    "    #print(grad)\n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'): #更新\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        #print(network.params['W2'])\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc,test_acc)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
